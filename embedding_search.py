# -*- coding: utf-8 -*-
"""Embedding_Search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zD9isQ3hycnWM_rP_L9Yw0kcEJC2X4JF
"""

from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from dotenv import load_dotenv
load_dotenv() 
import os
import asyncio

class VectorStoreManager:
    """
    Manages the creation of a FAISS vector store and a retriever from document chunks.
    """
    def __init__(self, embeddings: Embeddings):
        """
        Initializes the VectorStoreManager.

        Args:
            embeddings (Embeddings): The embedding model to use for vectorization.
        """
        self.embeddings = embeddings

    def create_and_save_store(self, documents: list, path : str ): 
        """Creates a FAISS vector store from documents and saves it."""
        print(f"Creating and saving new vector store to: {path}")

        # Create the FAISS vector store from the documents in memory
        vector_store = FAISS.from_documents(documents, self.embeddings)
        vector_store.save_local(path)
        print("Vector store saved successfully")

    def load_retriever(self, path : str, k: int = 10) -> VectorStoreRetriever:
        """Loads a retriever from a saved FAISS vector store."""
        print(f"Loading existing vector store from: {path}")
        vector_store = FAISS.load_local(
            path,
            self.embeddings,
            allow_dangerous_deserialization = True
        )
        return vector_store.as_retriever(search_kwargs = {"k": k})
    # ---------- Weighted search helper ----------------------------------------

import numpy as np
from typing import List

def _weighted_embed(
    base_text: str,
    key_phrase: str,
    embeddings: Embeddings,
    weight: float = 3.0
) -> List[float]:
    """
    Create a query vector that nudges the space toward `key_phrase`.
    """
    base_vec = np.array(embeddings.embed_query(base_text))
    key_vec  = np.array(embeddings.embed_query(key_phrase))
    combo    = base_vec + weight * key_vec          # simple linear mix
    combo   /= np.linalg.norm(combo)                # re-normalise
    return combo.tolist()


def weighted_similarity_search(
    vector_store: FAISS,
    embeddings: Embeddings,
    query: str,
    key_phrase: str,
    k: int = 10,
    weight: float = 3.0
):
    """
    One-off search that boosts `key_phrase` inside the vector space.
    Returns the same list[Document] you get from `similarity_search`.
    """
    q_vec = _weighted_embed(query, key_phrase, embeddings, weight)
    return vector_store.similarity_search_by_vector(q_vec, k=k)


# --- Test Execution ---
# --------------------------------------------------------------------------- #
# Quick standalone test: build index, parse query, show top-K clauses         #
# --------------------------------------------------------------------------- #
if __name__ == "__main__":
    from Reading_PDFBlobURLs import PDFProcessor
    from llm_parser import QueryParser
    from embedding_search import weighted_similarity_search   # ← helper added earlier

    # ---------- Config -----------------------------------------------------
    PDF_URL      = ("https://hackrx.blob.core.windows.net/assets/principia_newton.pdf?sv=2023-01-03&st=2025-07-28T07%3A20%3A32Z&se=2026-07-29T07%3A20%3A00Z&sr=b&sp=r&sig=V5I1QYyigoxeUMbnUKsdEaST99F5%2FDfo7wpKg9XXF5w%3D")
    SAMPLE_QUERY = "How does Newton derive Kepler's Second Law (equal areas in equal times) from his laws of motion and gravitation?"
    TOP_K_TOTAL  = 10
    KEYWORD_WEIGHT = 4.0

    # ---------- 1. Chunk the PDF ------------------------------------------
    pdf_processor = PDFProcessor()
    chunks = pdf_processor.load_and_chunk_from_url(PDF_URL)
    if not chunks:
        raise RuntimeError("PDF processing failed – no chunks returned.")

    # ---------- 2. Build FAISS + Retriever --------------------------------
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vdb = FAISS.from_documents(chunks, embeddings)
    retriever = vdb.as_retriever(search_kwargs={"k": TOP_K_TOTAL})

    # ---------- 3. Parse the sample query (async → sync) -------------------
    llm          = ChatOpenAI(temperature=0.0)
    query_parser = QueryParser(llm=llm)
    parsed       = asyncio.run(query_parser.parse_query(SAMPLE_QUERY))

    print("\nParsed query ➜", parsed.model_dump())

    # ---------- 4. Run weighted retrieval for ALL sub-queries -------------
    pooled, seen = [], set()
    for sq in parsed.sub_query or [parsed.raw_query]:
        docs = weighted_similarity_search(
            vdb,
            embeddings,
            sq.replace("_", " "),
            parsed.key_word,
            k=TOP_K_TOTAL,
            weight=KEYWORD_WEIGHT
        )
        for d in docs:
            sig = d.page_content[:120]
            if sig not in seen and len(pooled) < TOP_K_TOTAL:
                seen.add(sig)
                pooled.append(d)
        if len(pooled) >= TOP_K_TOTAL:
            break

    # ---------- 5. Display results ---------------------------------------
    print(f"\nTop-{len(pooled)} clauses for query: “{SAMPLE_QUERY}”\n"
          f"(keyword boost = '{parsed.key_word}', weight = {KEYWORD_WEIGHT})")
    for idx, doc in enumerate(pooled, 1):
        snippet = doc.page_content.replace("\n", " ")
        print(f"\n[{idx}] {snippet}")
