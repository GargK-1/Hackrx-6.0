# -*- coding: utf-8 -*-
"""Server.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gLYOpSrzj03N1ZerIdh0Z6al9qPpHMDc
"""

#pip install fastapi "uvicorn[standard]"  # uvicorn is a server used to run FastAPI

import asyncio
import json
import logging
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, AsyncGenerator
from contextlib import asynccontextmanager

from main_pipeline_savi2 import run_full_pipeline # import your pipeline
from Reading_PDFBlobURLsIMPROVED import PDFProcessor
from llm_parser_savi import QueryParser
from embedding_search_savi import VectorStoreManager
from clause_matching import DocumentReRanker
from logic_evaluation import AnswerGenerator
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
#from langchain_google_genai import ChatGoogleGenerativeAI

# --- 1. SET UP LOGGING ---
# This block configures the logging system to write to a file.
logging.basicConfig(
    filename='hackrx_requests.log', 
    level=logging.INFO, 
    format='%(asctime)s - %(message)s'
)


# 2. Create a dictionary to act as a cache for our models
model_cache = {}

# 3. Define the lifespan function to load models on startup
@asynccontextmanager
async def lifespan(app: FastAPI):
    # --- Code to run ON STARTUP ---
    print("INFO:     Server starting up...")
    print("INFO:     Loading models and components into cache...")
    
    # Load all heavy components once and store them in the cache
    #model_cache["llm"] = ChatGoogleGenerativeAI(model="models/gemini-2.5-flash")
    model_cache["llm"] = ChatOpenAI(temperature = 0.0, model_name = 'gpt-4o')
    model_cache["embeddings"] = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    model_cache["re_ranker"] = DocumentReRanker()
    
    # Initialize components that depend on the models
    model_cache["pdf_processor"] = PDFProcessor()
    model_cache["vector_store_manager"] = VectorStoreManager(embeddings=model_cache["embeddings"])
    model_cache["query_parser"] = QueryParser(llm=model_cache["llm"])
    model_cache["answer_generator"] = AnswerGenerator(llm=model_cache["llm"])
    
    print("INFO: Models and components loaded successfully.")
    
    yield # The server runs here
    
    # --- Code to run ON SHUTDOWN ---
    print("INFO:     Server shutting down...")
    model_cache.clear()

# 4. Create the FastAPI app and pass it the lifespan manager
app = FastAPI(lifespan=lifespan)

class HackRxRequest(BaseModel):
    documents: str
    questions: List[str]

# This is the async generator that implements the streaming logic
async def stream_pipeline_response(pdf_url: str, questions: list[str], components: dict) -> AsyncGenerator[str, None]:
    """
    Runs the pipeline in the background while sending keep-alive pings.
    """
    # Create a background task to run the long pipeline
    pipeline_task = asyncio.create_task(
        run_full_pipeline(pdf_url, questions, components)
    )

    # Send a keep-alive ping every 10 seconds until the pipeline is done
    ping_count = 0
    while not pipeline_task.done():
        ping_count += 1 
        print(f"--- Sending keep-alive ping #{ping_count} ---")
        yield " \n"  # Send a newline character character to keep the connection alive
        await asyncio.sleep(5)

    # Once the pipeline is finished, get the result
    result = await pipeline_task
    
    # Send the final JSON result as the last chunk
    yield json.dumps(result)

# Your existing health check endpoint (no changes needed)
@app.get("/api/v1/hackrx/run")
async def hackrx_run_health():
    return {"status": "ready"}

@app.post("/api/v1/hackrx/run")
async def handle_submission(request_data: HackRxRequest):
    print("\n--- Received request from HackRx server. ---")
    # --- ADD THIS BLOCK TO PRINT THE INCOMING URL ---
    print("\n" + "="*50)
    print("RECEIVED NEW REQUEST DATA:")
    print(f"  Document URL: {request_data.documents}")
    print(f"  Questions: {request_data.questions}")
    print("="*50 + "\n")
    # --- END OF ADDITION ---

    # Log the incoming request data to a file for a permanent record
    logging.info(f"Received documents URL: {request_data.documents}")
    logging.info(f"Received questions: {request_data.questions}")
    
    try:

        # Return a StreamingResponse that calls our async generator
        # return StreamingResponse(
        #     stream_pipeline_response(
        #         pdf_url=request_data.documents,
        #         questions=request_data.questions,
        #         components=model_cache
        #     ),
        #     media_type="application/json"
        # )

        
        # 5. Pass the pre-loaded components from the cache to the pipeline
        result = await run_full_pipeline(
            pdf_url=request_data.documents, 
            questions=request_data.questions,
            components=model_cache # Pass the cache
        )

        # --- ADD THIS BLOCK TO LOG THE FINAL ANSWERS ---
        print("\n" + "="*50)
        print("--- FINAL RESPONSE PAYLOAD ---")
        # Pretty-print the JSON to the console for easy reading
        print(json.dumps(result, indent=2))
        print("="*50)
        
        # Log the same result to your 'hackrx_requests.log' file for a permanent record
        logging.info(f"Sending final response: {result}")
        # --- END OF LOGGING BLOCK ---

        print("--- Pipeline finished. Sending back the response. ---")
        return result
    except Exception as e:
        print(f"An error occurred during pipeline execution: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
def read_root():
    return {"status": "API is running and models are loaded."}

# # 1. Define a Pydantic model to validate the incoming request body.
# # This ensures the data from HackRx has the correct structure.
# class HackRxRequest(BaseModel):
#     documents: str
#     questions: List[str]

# # 2. Initialize the FastAPI application
# app = FastAPI()

# # Health check for the competition portal 
# @app.get("/api/v1/hackrx/run")
# async def hackrx_run_health():
#     return {"status": "ready"}   # HTTP 200


# # 3. Define the POST endpoint that HackRx will call
# # The path "/api/v1/hackrx/run" is based on the example URL structure.
# @app.post("/api/v1/hackrx/run")
# @app.post("/hackrx/run")
# async def handle_submission(request_data: HackRxRequest):
#     print("--- 1. Request received by FastAPI server. ---")
#     """
#     This function is triggered when a POST request is received at this endpoint.
#     """
#     print("Received request from HackRx server...")
#     try:
#         # Extract data from the validated request
#         pdf_url = request_data.documents
#         questions = request_data.questions

#         # Run your entire existing pipeline with the received data
#         result = await run_full_pipeline(pdf_url, questions)

#         print("Pipeline finished. Sending back the response.")
#         # FastAPI automatically converts this dictionary to a JSON response
#         return result

#     except Exception as e:
#         # Handle any potential errors during processing
#         print(f"An error occurred during pipeline execution: {e}")
#         raise HTTPException(status_code=500, detail=str(e))

# @app.get("/")
# def read_root():
#     return {"status": "API is running"}

